"""
AI Model Accuracy Verification Tests
AI ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
ä½œæˆæ—¥: 2025-06-20
"""

import pytest
import json
import time
import statistics
from typing import List, Dict, Any, Tuple
from datetime import datetime
import pandas as pd

class AIModelTester:
    """AI ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_endpoint: str = "http://localhost:8000"):
        self.endpoint = model_endpoint
        self.test_results = []
        
    def measure_response_time(self, func, *args, **kwargs) -> Tuple[Any, float]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¸¬å®š"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return result, (end_time - start_time) * 1000  # ms

    def calculate_accuracy(self, predictions: List[Any], ground_truth: List[Any]) -> float:
        """ç²¾åº¦ã‚’è¨ˆç®—"""
        if len(predictions) != len(ground_truth):
            raise ValueError("Predictions and ground truth must have same length")
        
        correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)
        return correct / len(predictions) * 100

    def calculate_similarity_score(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 and not words2:
            return 100.0
        if not words1 or not words2:
            return 0.0
            
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return (intersection / union) * 100

class DocumentGenerationTester(AIModelTester):
    """æ›¸é¡ç”Ÿæˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_data = self.load_test_data()
        
    def load_test_data(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        return {
            "companies": [
                {
                    "name": "æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆè£½é€ ",
                    "business_type": "è£½é€ æ¥­",
                    "employee_count": 50,
                    "annual_revenue": 500000000,
                    "capital": 50000000
                },
                {
                    "name": "ä¸­å°ITä¼æ¥­æ ªå¼ä¼šç¤¾", 
                    "business_type": "æƒ…å ±é€šä¿¡æ¥­",
                    "employee_count": 30,
                    "annual_revenue": 200000000,
                    "capital": 10000000
                }
            ],
            "projects": [
                {
                    "name": "DXæ¨é€²ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
                    "description": "è£½é€ å·¥ç¨‹ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
                    "budget": 10000000,
                    "duration": 12,
                    "technology": ["IoT", "AI", "ã‚¯ãƒ©ã‚¦ãƒ‰"]
                }
            ],
            "expected_outputs": {
                "estimate_sheet": {
                    "sections": ["äººä»¶è²»", "è¨­å‚™è²»", "ãã®ä»–çµŒè²»"],
                    "total_amount_range": (8000000, 12000000),
                    "required_fields": ["é …ç›®", "å˜ä¾¡", "æ•°é‡", "é‡‘é¡"]
                },
                "business_plan": {
                    "sections": ["äº‹æ¥­æ¦‚è¦", "å¸‚å ´åˆ†æ", "å®Ÿæ–½è¨ˆç”»", "æœŸå¾…åŠ¹æœ"],
                    "min_length": 1000,
                    "required_keywords": ["ç”Ÿç”£æ€§å‘ä¸Š", "ã‚³ã‚¹ãƒˆå‰Šæ¸›", "ç«¶äº‰åŠ›"]
                }
            }
        }

    def test_estimate_sheet_generation(self) -> Dict[str, Any]:
        """è¦‹ç©æ›¸ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        results = []
        
        for company in self.test_data["companies"]:
            for project in self.test_data["projects"]:
                # è¦‹ç©æ›¸ç”Ÿæˆã®å®Ÿè¡Œ
                generation_data = {
                    "document_type": "estimate_sheet",
                    "company_info": company,
                    "project_info": project
                }
                
                generated_doc, response_time = self.measure_response_time(
                    self._call_generation_api, generation_data
                )
                
                # ç²¾åº¦è©•ä¾¡
                accuracy_scores = self._evaluate_estimate_accuracy(generated_doc)
                
                result = {
                    "company": company["name"],
                    "project": project["name"],
                    "response_time_ms": response_time,
                    "accuracy_scores": accuracy_scores,
                    "generated_document": generated_doc
                }
                results.append(result)
        
        return self._summarize_results(results)

    def test_business_plan_generation(self) -> Dict[str, Any]:
        """äº‹æ¥­è¨ˆç”»æ›¸ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        results = []
        
        for company in self.test_data["companies"]:
            for project in self.test_data["projects"]:
                generation_data = {
                    "document_type": "business_plan",
                    "company_info": company,
                    "project_info": project
                }
                
                generated_doc, response_time = self.measure_response_time(
                    self._call_generation_api, generation_data
                )
                
                # å†…å®¹ã®è³ªçš„è©•ä¾¡
                quality_scores = self._evaluate_business_plan_quality(generated_doc)
                
                result = {
                    "company": company["name"],
                    "project": project["name"], 
                    "response_time_ms": response_time,
                    "quality_scores": quality_scores,
                    "document_length": len(generated_doc.get("content", "")),
                    "generated_document": generated_doc
                }
                results.append(result)
        
        return self._summarize_results(results)

    def _call_generation_api(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ›¸é¡ç”ŸæˆAPIã®å‘¼ã³å‡ºã—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ requests.post() ãªã©ã‚’ä½¿ç”¨
        import random
        
        if data["document_type"] == "estimate_sheet":
            return {
                "content": self._generate_mock_estimate(data),
                "status": "success",
                "sections": ["äººä»¶è²»", "è¨­å‚™è²»", "ãã®ä»–çµŒè²»"],
                "total_amount": random.randint(8000000, 12000000)
            }
        elif data["document_type"] == "business_plan":
            return {
                "content": self._generate_mock_business_plan(data),
                "status": "success",
                "sections": ["äº‹æ¥­æ¦‚è¦", "å¸‚å ´åˆ†æ", "å®Ÿæ–½è¨ˆç”»", "æœŸå¾…åŠ¹æœ"],
                "word_count": random.randint(1000, 2000)
            }

    def _generate_mock_estimate(self, data: Dict[str, Any]) -> str:
        """ãƒ¢ãƒƒã‚¯è¦‹ç©æ›¸ç”Ÿæˆ"""
        company = data["company_info"]["name"]
        project = data["project_info"]["name"]
        
        return f"""
        è¦‹ç©æ›¸
        
        ä»¶å: {project}
        ç™ºæ³¨è€…: {company}
        
        â– äººä»¶è²»
        - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: 1,000,000å††
        - ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢: 3,000,000å††
        - ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼: 2,000,000å††
        
        â– è¨­å‚™è²»
        - ã‚µãƒ¼ãƒãƒ¼è²»ç”¨: 2,000,000å††
        - ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: 1,500,000å††
        
        â– ãã®ä»–çµŒè²»
        - äº¤é€šè²»ãƒ»é›‘è²»: 500,000å††
        
        åˆè¨ˆ: 10,000,000å††
        """

    def _generate_mock_business_plan(self, data: Dict[str, Any]) -> str:
        """ãƒ¢ãƒƒã‚¯äº‹æ¥­è¨ˆç”»æ›¸ç”Ÿæˆ"""
        company = data["company_info"]["name"]
        project = data["project_info"]["name"]
        
        return f"""
        äº‹æ¥­è¨ˆç”»æ›¸
        
        â– äº‹æ¥­æ¦‚è¦
        {company}ã¯{project}ã‚’å®Ÿæ–½ã—ã€ç”Ÿç”£æ€§å‘ä¸Šã¨ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
        æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ˆã‚Šã€ç«¶äº‰åŠ›ã®å¼·åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚
        
        â– å¸‚å ´åˆ†æ
        ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã®é€²å±•ã«ã‚ˆã‚Šã€è£½é€ æ¥­ã®DXéœ€è¦ãŒæ‹¡å¤§ã—ã¦ã„ã¾ã™ã€‚
        å½“ç¤¾ã®æŠ€è¡“ã«ã‚ˆã‚Šã€å¸‚å ´ã§ã®å„ªä½æ€§ã‚’ç¢ºç«‹ã§ãã¾ã™ã€‚
        
        â– å®Ÿæ–½è¨ˆç”»
        12ãƒ¶æœˆã®æœŸé–“ã§æ®µéšçš„ã«ã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã—ã€å¾“æ¥­å“¡æ•™è‚²ã‚‚ä¸¦è¡Œã—ã¦å®Ÿæ–½ã—ã¾ã™ã€‚
        
        â– æœŸå¾…åŠ¹æœ
        - ç”Ÿç”£æ€§å‘ä¸Š: 20%
        - ã‚³ã‚¹ãƒˆå‰Šæ¸›: 15%
        - ç«¶äº‰åŠ›å¼·åŒ–ã«ã‚ˆã‚‹å£²ä¸Šå¢—åŠ 
        """

    def _evaluate_estimate_accuracy(self, generated_doc: Dict[str, Any]) -> Dict[str, float]:
        """è¦‹ç©æ›¸ã®ç²¾åº¦è©•ä¾¡"""
        expected = self.test_data["expected_outputs"]["estimate_sheet"]
        
        # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å­˜åœ¨ç¢ºèª
        sections_score = 0
        if "sections" in generated_doc:
            matching_sections = set(generated_doc["sections"]) & set(expected["sections"])
            sections_score = len(matching_sections) / len(expected["sections"]) * 100
        
        # é‡‘é¡å¦¥å½“æ€§ã®ç¢ºèª
        amount_score = 0
        if "total_amount" in generated_doc:
            amount = generated_doc["total_amount"]
            min_amount, max_amount = expected["total_amount_range"]
            if min_amount <= amount <= max_amount:
                amount_score = 100
            else:
                # ç¯„å›²å¤–ã®å ´åˆã¯è·é›¢ã«å¿œã˜ã¦æ¸›ç‚¹
                if amount < min_amount:
                    amount_score = max(0, 100 - (min_amount - amount) / min_amount * 100)
                else:
                    amount_score = max(0, 100 - (amount - max_amount) / max_amount * 100)
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
        fields_score = 0
        content = generated_doc.get("content", "")
        matching_fields = sum(1 for field in expected["required_fields"] if field in content)
        fields_score = matching_fields / len(expected["required_fields"]) * 100
        
        return {
            "sections_accuracy": sections_score,
            "amount_validity": amount_score,
            "required_fields": fields_score,
            "overall_accuracy": (sections_score + amount_score + fields_score) / 3
        }

    def _evaluate_business_plan_quality(self, generated_doc: Dict[str, Any]) -> Dict[str, float]:
        """äº‹æ¥­è¨ˆç”»æ›¸ã®å“è³ªè©•ä¾¡"""
        expected = self.test_data["expected_outputs"]["business_plan"]
        
        # æ–‡å­—æ•°ã®ç¢ºèª
        content = generated_doc.get("content", "")
        length_score = min(100, len(content) / expected["min_length"] * 100)
        
        # å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºèª
        keyword_score = 0
        matching_keywords = sum(1 for keyword in expected["required_keywords"] if keyword in content)
        keyword_score = matching_keywords / len(expected["required_keywords"]) * 100
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã®ç¢ºèª
        sections_score = 0
        if "sections" in generated_doc:
            matching_sections = set(generated_doc["sections"]) & set(expected["sections"])
            sections_score = len(matching_sections) / len(expected["sections"]) * 100
        
        # è«–ç†çš„æ•´åˆæ€§ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
        coherence_score = self._check_coherence(content)
        
        return {
            "length_adequacy": length_score,
            "keyword_coverage": keyword_score,
            "section_completeness": sections_score,
            "logical_coherence": coherence_score,
            "overall_quality": (length_score + keyword_score + sections_score + coherence_score) / 4
        }

    def _check_coherence(self, content: str) -> float:
        """è«–ç†çš„æ•´åˆæ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯"""
        # æ®µè½ã®æ•°ã¨æ¥ç¶šè©ã®ä½¿ç”¨ã‚’ãƒã‚§ãƒƒã‚¯
        paragraphs = content.split('\n\n')
        
        if len(paragraphs) < 3:
            return 50  # æ®µè½ãŒå°‘ãªã™ãã‚‹
        
        # æ¥ç¶šè©ã®ç¢ºèª
        connectors = ["ã¾ãŸ", "ã•ã‚‰ã«", "ä¸€æ–¹", "ã—ã‹ã—", "ãã®ãŸã‚", "ã—ãŸãŒã£ã¦"]
        connector_count = sum(1 for connector in connectors if connector in content)
        
        coherence_score = min(100, (connector_count / len(paragraphs)) * 100 + 50)
        return coherence_score

    def _summarize_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        if not results:
            return {"error": "No test results"}
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®çµ±è¨ˆ
        response_times = [r["response_time_ms"] for r in results]
        
        # ç²¾åº¦ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ
        if "accuracy_scores" in results[0]:
            accuracy_scores = [r["accuracy_scores"]["overall_accuracy"] for r in results]
        else:
            accuracy_scores = [r["quality_scores"]["overall_quality"] for r in results]
        
        return {
            "test_count": len(results),
            "response_time_stats": {
                "average": statistics.mean(response_times),
                "median": statistics.median(response_times),
                "max": max(response_times),
                "min": min(response_times)
            },
            "accuracy_stats": {
                "average": statistics.mean(accuracy_scores),
                "median": statistics.median(accuracy_scores),
                "max": max(accuracy_scores),
                "min": min(accuracy_scores)
            },
            "detailed_results": results
        }

class GuidelineParsingTester(AIModelTester):
    """å‹Ÿé›†è¦é …è§£ææ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_guidelines = self.load_test_guidelines()
    
    def load_test_guidelines(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆç”¨å‹Ÿé›†è¦é …ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        return [
            {
                "name": "ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘ï¼ˆä¸€èˆ¬å‹ï¼‰",
                "content": """
                ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘ï¼ˆä¸€èˆ¬å‹ï¼‰
                
                1. è£œåŠ©å¯¾è±¡è€…
                ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…
                
                2. è£œåŠ©ç‡ãƒ»è£œåŠ©ä¸Šé™é¡
                è£œåŠ©ç‡ï¼š1/2
                è£œåŠ©ä¸Šé™é¡ï¼š1,250ä¸‡å††
                
                3. è£œåŠ©å¯¾è±¡çµŒè²»
                æ©Ÿæ¢°è£…ç½®ãƒ»ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰è²»
                æŠ€è¡“å°å…¥è²»
                å°‚é–€å®¶çµŒè²»
                
                4. å®Ÿæ–½æœŸé–“
                äº¤ä»˜æ±ºå®šæ—¥ã‹ã‚‰10ã‹æœˆä»¥å†…
                """,
                "expected_structure": {
                    "subsidy_name": "ã‚‚ã®ã¥ãã‚Šè£œåŠ©é‡‘ï¼ˆä¸€èˆ¬å‹ï¼‰",
                    "target_entities": ["ä¸­å°ä¼æ¥­", "å°è¦æ¨¡äº‹æ¥­è€…"],
                    "subsidy_rate": 50,
                    "max_amount": 12500000,
                    "eligible_expenses": ["æ©Ÿæ¢°è£…ç½®ãƒ»ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰è²»", "æŠ€è¡“å°å…¥è²»", "å°‚é–€å®¶çµŒè²»"],
                    "implementation_period": "10ã‹æœˆ"
                }
            },
            {
                "name": "ITå°å…¥è£œåŠ©é‡‘ï¼ˆAé¡å‹ï¼‰",
                "content": """
                ITå°å…¥è£œåŠ©é‡‘ 2024 Aé¡å‹
                
                â– å¯¾è±¡
                ä¸­å°ä¼æ¥­ãƒ»å°è¦æ¨¡äº‹æ¥­è€…ç­‰
                
                â– è£œåŠ©é¡ãƒ»è£œåŠ©ç‡
                5ä¸‡å††ï½150ä¸‡å††æœªæº€ï¼ˆ1/2ä»¥å†…ï¼‰
                
                â– å¯¾è±¡çµŒè²»
                ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è³¼å…¥è²»
                å°å…¥é–¢é€£è²»
                
                â– å®Ÿæ–½æœŸé–“
                äº¤ä»˜æ±ºå®šï½11ã‹æœˆå¾Œ
                """,
                "expected_structure": {
                    "subsidy_name": "ITå°å…¥è£œåŠ©é‡‘ï¼ˆAé¡å‹ï¼‰",
                    "target_entities": ["ä¸­å°ä¼æ¥­", "å°è¦æ¨¡äº‹æ¥­è€…"],
                    "subsidy_rate": 50,
                    "max_amount": 1500000,
                    "min_amount": 50000,
                    "eligible_expenses": ["ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è³¼å…¥è²»", "å°å…¥é–¢é€£è²»"],
                    "implementation_period": "11ã‹æœˆ"
                }
            }
        ]

    def test_parsing_accuracy(self) -> Dict[str, Any]:
        """è§£æç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        results = []
        
        for guideline in self.test_guidelines:
            # è§£æå®Ÿè¡Œ
            parsed_result, response_time = self.measure_response_time(
                self._call_parsing_api, guideline["content"]
            )
            
            # ç²¾åº¦è©•ä¾¡
            accuracy_scores = self._evaluate_parsing_accuracy(
                parsed_result, guideline["expected_structure"]
            )
            
            result = {
                "guideline_name": guideline["name"],
                "response_time_ms": response_time,
                "accuracy_scores": accuracy_scores,
                "parsed_result": parsed_result,
                "expected": guideline["expected_structure"]
            }
            results.append(result)
        
        return self._summarize_results(results)

    def test_parsing_robustness(self) -> Dict[str, Any]:
        """è§£æã®å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆï¼ˆãƒã‚¤ã‚ºå«ã‚€ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        results = []
        
        for guideline in self.test_guidelines:
            # ãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            noisy_content = self._add_noise(guideline["content"])
            
            try:
                parsed_result, response_time = self.measure_response_time(
                    self._call_parsing_api, noisy_content
                )
                
                # åŸºæœ¬çš„ãªæ§‹é€ ãŒè§£æã§ãã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                robustness_score = self._evaluate_robustness(
                    parsed_result, guideline["expected_structure"]
                )
                
                result = {
                    "guideline_name": f"{guideline['name']}_noisy",
                    "response_time_ms": response_time,
                    "robustness_score": robustness_score,
                    "success": True
                }
            except Exception as e:
                result = {
                    "guideline_name": f"{guideline['name']}_noisy",
                    "response_time_ms": 0,
                    "robustness_score": 0,
                    "success": False,
                    "error": str(e)
                }
            
            results.append(result)
        
        return self._summarize_robustness_results(results)

    def _call_parsing_api(self, content: str) -> Dict[str, Any]:
        """è§£æAPIã®å‘¼ã³å‡ºã—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ AIè§£æã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—
        import re
        
        result = {}
        
        # è£œåŠ©é‡‘åã®æŠ½å‡º
        name_match = re.search(r'(.*?è£œåŠ©é‡‘.*?)(?:\n|$)', content)
        if name_match:
            result["subsidy_name"] = name_match.group(1).strip()
        
        # è£œåŠ©ç‡ã®æŠ½å‡º
        rate_match = re.search(r'(\d+)/(\d+)|(\d+)%', content)
        if rate_match:
            if rate_match.group(1) and rate_match.group(2):
                result["subsidy_rate"] = int(rate_match.group(1)) / int(rate_match.group(2)) * 100
            elif rate_match.group(3):
                result["subsidy_rate"] = int(rate_match.group(3))
        
        # é‡‘é¡ã®æŠ½å‡º
        amount_matches = re.findall(r'(\d+(?:,\d+)*)ä¸‡å††', content)
        if amount_matches:
            amounts = [int(match.replace(',', '')) * 10000 for match in amount_matches]
            result["max_amount"] = max(amounts)
            if len(amounts) > 1:
                result["min_amount"] = min(amounts)
        
        # å¯¾è±¡è€…ã®æŠ½å‡º
        if "ä¸­å°ä¼æ¥­" in content:
            result["target_entities"] = ["ä¸­å°ä¼æ¥­"]
            if "å°è¦æ¨¡äº‹æ¥­è€…" in content:
                result["target_entities"].append("å°è¦æ¨¡äº‹æ¥­è€…")
        
        # å®Ÿæ–½æœŸé–“ã®æŠ½å‡º
        period_match = re.search(r'(\d+)(?:ã‹|ãƒ¶)æœˆ', content)
        if period_match:
            result["implementation_period"] = f"{period_match.group(1)}ã‹æœˆ"
        
        return result

    def _evaluate_parsing_accuracy(self, parsed: Dict[str, Any], expected: Dict[str, Any]) -> Dict[str, float]:
        """è§£æç²¾åº¦ã®è©•ä¾¡"""
        field_scores = {}
        
        for field, expected_value in expected.items():
            if field in parsed:
                parsed_value = parsed[field]
                
                if isinstance(expected_value, str):
                    # æ–‡å­—åˆ—ã®é¡ä¼¼åº¦
                    similarity = self.calculate_similarity_score(str(parsed_value), expected_value)
                    field_scores[field] = similarity
                elif isinstance(expected_value, (int, float)):
                    # æ•°å€¤ã®ä¸€è‡´åº¦
                    if abs(parsed_value - expected_value) / expected_value < 0.1:  # 10%ä»¥å†…
                        field_scores[field] = 100
                    else:
                        field_scores[field] = max(0, 100 - abs(parsed_value - expected_value) / expected_value * 100)
                elif isinstance(expected_value, list):
                    # ãƒªã‚¹ãƒˆã®ä¸€è‡´åº¦
                    if isinstance(parsed_value, list):
                        intersection = set(parsed_value) & set(expected_value)
                        field_scores[field] = len(intersection) / len(expected_value) * 100
                    else:
                        field_scores[field] = 0
                else:
                    field_scores[field] = 100 if parsed_value == expected_value else 0
            else:
                field_scores[field] = 0  # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæŠ½å‡ºã•ã‚Œã¦ã„ãªã„
        
        overall_accuracy = sum(field_scores.values()) / len(field_scores) if field_scores else 0
        
        return {
            **field_scores,
            "overall_accuracy": overall_accuracy
        }

    def _add_noise(self, content: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã«ãƒã‚¤ã‚ºã‚’è¿½åŠ """
        # ã‚¿ã‚¤ãƒã€ä½™åˆ†ãªæ”¹è¡Œã€ç‰¹æ®Šæ–‡å­—ãªã©ã‚’è¿½åŠ 
        noisy_content = content.replace("è£œåŠ©é‡‘", "è£œ åŠ© é‡‘")  # ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ 
        noisy_content = noisy_content.replace("ä¸‡å††", "ä¸‡å††ã€€")  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ 
        noisy_content = f"ã€é‡è¦ã€‘\n{noisy_content}\nâ€»ã“ã®æ–‡æ›¸ã¯æ­£å¼ãªã‚‚ã®ã§ã™ã€‚"  # ä½™åˆ†ãªæƒ…å ±è¿½åŠ 
        return noisy_content

    def _evaluate_robustness(self, parsed: Dict[str, Any], expected: Dict[str, Any]) -> float:
        """å …ç‰¢æ€§ã®è©•ä¾¡ï¼ˆåŸºæœ¬çš„ãªæƒ…å ±ãŒå–ã‚Œã¦ã„ã‚‹ã‹ï¼‰"""
        essential_fields = ["subsidy_name", "subsidy_rate", "max_amount"]
        
        extracted_essential = sum(1 for field in essential_fields if field in parsed)
        return extracted_essential / len(essential_fields) * 100

    def _summarize_robustness_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼"""
        success_count = sum(1 for r in results if r["success"])
        
        if success_count > 0:
            successful_results = [r for r in results if r["success"]]
            avg_robustness = statistics.mean([r["robustness_score"] for r in successful_results])
        else:
            avg_robustness = 0
        
        return {
            "total_tests": len(results),
            "successful_tests": success_count,
            "success_rate": success_count / len(results) * 100,
            "average_robustness_score": avg_robustness,
            "detailed_results": results
        }

class ProgressPredictionTester(AIModelTester):
    """é€²æ—äºˆæ¸¬æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_scenarios = self.create_test_scenarios()
    
    def create_test_scenarios(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®ä½œæˆ"""
        return [
            {
                "name": "é †èª¿ãªé€²æ—",
                "project_data": {
                    "planned_milestones": [
                        {"name": "è¦ä»¶å®šç¾©", "planned_date": "2025-08-31", "actual_date": "2025-08-30"},
                        {"name": "è¨­è¨ˆ", "planned_date": "2025-10-31", "actual_date": None},
                        {"name": "é–‹ç™º", "planned_date": "2026-03-31", "actual_date": None},
                        {"name": "ãƒ†ã‚¹ãƒˆ", "planned_date": "2026-05-31", "actual_date": None}
                    ],
                    "current_date": "2025-09-15",
                    "budget_consumption": 0.15,  # 15% æ¶ˆåŒ–
                    "team_velocity": 1.1  # 110% of planned
                },
                "expected_prediction": {
                    "delay_probability": "low",
                    "estimated_completion": "2026-05-25",
                    "risk_level": "low"
                }
            },
            {
                "name": "é…å»¶ãƒªã‚¹ã‚¯ã‚ã‚Š",
                "project_data": {
                    "planned_milestones": [
                        {"name": "è¦ä»¶å®šç¾©", "planned_date": "2025-08-31", "actual_date": "2025-09-15"},
                        {"name": "è¨­è¨ˆ", "planned_date": "2025-10-31", "actual_date": None},
                        {"name": "é–‹ç™º", "planned_date": "2026-03-31", "actual_date": None},
                        {"name": "ãƒ†ã‚¹ãƒˆ", "planned_date": "2026-05-31", "actual_date": None}
                    ],
                    "current_date": "2025-10-01",
                    "budget_consumption": 0.35,  # 35% æ¶ˆåŒ–ï¼ˆæƒ³å®šã‚ˆã‚Šå¤šã„ï¼‰
                    "team_velocity": 0.8  # 80% of planned
                },
                "expected_prediction": {
                    "delay_probability": "high",
                    "estimated_completion": "2026-07-15",
                    "risk_level": "high"
                }
            }
        ]

    def test_prediction_accuracy(self) -> Dict[str, Any]:
        """äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        results = []
        
        for scenario in self.test_scenarios:
            # äºˆæ¸¬å®Ÿè¡Œ
            prediction, response_time = self.measure_response_time(
                self._call_prediction_api, scenario["project_data"]
            )
            
            # äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡
            accuracy_score = self._evaluate_prediction_accuracy(
                prediction, scenario["expected_prediction"]
            )
            
            result = {
                "scenario_name": scenario["name"],
                "response_time_ms": response_time,
                "accuracy_score": accuracy_score,
                "prediction": prediction,
                "expected": scenario["expected_prediction"]
            }
            results.append(result)
        
        return self._summarize_results(results)

    def _call_prediction_api(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """äºˆæ¸¬APIã®å‘¼ã³å‡ºã—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        # ç°¡å˜ãªäºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
        team_velocity = project_data.get("team_velocity", 1.0)
        budget_consumption = project_data.get("budget_consumption", 0.0)
        
        # é…å»¶ç¢ºç‡ã®è¨ˆç®—
        delay_factor = (1 - team_velocity) + (budget_consumption - 0.2)  # äºˆç®—20%ã‚’åŸºæº–
        
        if delay_factor < 0.1:
            delay_probability = "low"
            risk_level = "low"
            delay_months = 0
        elif delay_factor < 0.3:
            delay_probability = "medium"
            risk_level = "medium"
            delay_months = 1
        else:
            delay_probability = "high"
            risk_level = "high"
            delay_months = 2
        
        # å®Œäº†äºˆå®šæ—¥ã®èª¿æ•´ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        from datetime import datetime, timedelta
        base_completion = datetime(2026, 5, 31)
        estimated_completion = base_completion + timedelta(days=delay_months * 30)
        
        return {
            "delay_probability": delay_probability,
            "estimated_completion": estimated_completion.strftime("%Y-%m-%d"),
            "risk_level": risk_level,
            "confidence": 0.85
        }

    def _evaluate_prediction_accuracy(self, predicted: Dict[str, Any], expected: Dict[str, Any]) -> float:
        """äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡"""
        scores = []
        
        # é…å»¶ç¢ºç‡ã®ä¸€è‡´
        if predicted.get("delay_probability") == expected.get("delay_probability"):
            scores.append(100)
        else:
            scores.append(50)  # éƒ¨åˆ†ç‚¹
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´
        if predicted.get("risk_level") == expected.get("risk_level"):
            scores.append(100)
        else:
            scores.append(50)
        
        # å®Œäº†äºˆå®šæ—¥ã®ç²¾åº¦ï¼ˆÂ±1ãƒ¶æœˆä»¥å†…ãªã‚‰æº€ç‚¹ï¼‰
        pred_date = datetime.strptime(predicted.get("estimated_completion", ""), "%Y-%m-%d")
        exp_date = datetime.strptime(expected.get("estimated_completion", ""), "%Y-%m-%d")
        
        date_diff = abs((pred_date - exp_date).days)
        if date_diff <= 30:  # 1ãƒ¶æœˆä»¥å†…
            scores.append(100)
        elif date_diff <= 60:  # 2ãƒ¶æœˆä»¥å†…
            scores.append(70)
        else:
            scores.append(30)
        
        return sum(scores) / len(scores)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°
def run_all_ai_tests():
    """å…¨ã¦ã®AIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    results = {}
    
    print("ğŸ¤– AI Model Accuracy Tests Started")
    
    # æ›¸é¡ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ“„ Testing Document Generation...")
    doc_tester = DocumentGenerationTester()
    results["estimate_generation"] = doc_tester.test_estimate_sheet_generation()
    results["business_plan_generation"] = doc_tester.test_business_plan_generation()
    
    # å‹Ÿé›†è¦é …è§£æãƒ†ã‚¹ãƒˆ
    print("\nğŸ“‹ Testing Guideline Parsing...")
    parsing_tester = GuidelineParsingTester()
    results["parsing_accuracy"] = parsing_tester.test_parsing_accuracy()
    results["parsing_robustness"] = parsing_tester.test_parsing_robustness()
    
    # é€²æ—äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“Š Testing Progress Prediction...")
    prediction_tester = ProgressPredictionTester()
    results["progress_prediction"] = prediction_tester.test_prediction_accuracy()
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“ˆ Generating Comprehensive Report...")
    comprehensive_report = generate_comprehensive_report(results)
    
    return comprehensive_report

def generate_comprehensive_report(results: Dict[str, Any]) -> Dict[str, Any]:
    """åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report = {
        "test_summary": {
            "total_test_categories": len(results),
            "execution_timestamp": datetime.now().isoformat()
        },
        "performance_summary": {},
        "accuracy_summary": {},
        "recommendations": []
    }
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
    all_response_times = []
    for category, result in results.items():
        if "response_time_stats" in result:
            all_response_times.extend([result["response_time_stats"]["average"]])
    
    if all_response_times:
        report["performance_summary"] = {
            "average_response_time_ms": statistics.mean(all_response_times),
            "max_response_time_ms": max(all_response_times),
            "performance_grade": "A" if max(all_response_times) < 5000 else "B" if max(all_response_times) < 10000 else "C"
        }
    
    # ç²¾åº¦ã‚µãƒãƒªãƒ¼
    all_accuracy_scores = []
    for category, result in results.items():
        if "accuracy_stats" in result:
            all_accuracy_scores.append(result["accuracy_stats"]["average"])
    
    if all_accuracy_scores:
        overall_accuracy = statistics.mean(all_accuracy_scores)
        report["accuracy_summary"] = {
            "overall_accuracy_percentage": overall_accuracy,
            "accuracy_grade": "A" if overall_accuracy >= 90 else "B" if overall_accuracy >= 80 else "C"
        }
    
    # æ¨å¥¨äº‹é …
    if report["performance_summary"].get("performance_grade", "A") != "A":
        report["recommendations"].append("Response time optimization needed for better user experience")
    
    if report["accuracy_summary"].get("accuracy_grade", "A") != "A":
        report["recommendations"].append("Model fine-tuning required to improve accuracy")
    
    report["detailed_results"] = results
    
    return report

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_results = run_all_ai_tests()
    
    # çµæœã®å‡ºåŠ›
    print("\n" + "="*50)
    print("ğŸ¯ AI MODEL TEST RESULTS SUMMARY")
    print("="*50)
    
    performance = test_results.get("performance_summary", {})
    accuracy = test_results.get("accuracy_summary", {})
    
    print(f"ğŸ“Š Overall Performance Grade: {performance.get('performance_grade', 'N/A')}")
    print(f"ğŸ¯ Overall Accuracy Grade: {accuracy.get('accuracy_grade', 'N/A')}")
    print(f"âš¡ Average Response Time: {performance.get('average_response_time_ms', 0):.2f}ms")
    print(f"âœ… Average Accuracy: {accuracy.get('overall_accuracy_percentage', 0):.2f}%")
    
    if test_results.get("recommendations"):
        print("\nğŸ’¡ Recommendations:")
        for rec in test_results["recommendations"]:
            print(f"  â€¢ {rec}")
    
    print("\nâœ… All AI model tests completed successfully!")